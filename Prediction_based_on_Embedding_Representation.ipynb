{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction based on Embedding Representation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunitavni/Deep-Learning/blob/master/Prediction_based_on_Embedding_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ekmjkIWu2Qe",
        "colab_type": "text"
      },
      "source": [
        "#Tennis match prediction - Deep Learning\n",
        "\n",
        "In this project I developed an algorithm that predicts the winner of a given Tennis match.\n",
        "\n",
        "In order to do that, I will be working on two networks:\n",
        "1. Embedding network - trained to predict a single outcome, given two players, in order to learn how to represent a player using a vector.\n",
        "2. Prediction network - given two tennis player, this network predicts who is about to win. Each player is represented by a vector that is learned by the embedding network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJSm3DM8u5mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "5e5dd9c6-3780-40ec-9ce5-fff4b7211125"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pathlib import Path\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1RNCS-0w95q",
        "colab_type": "text"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "The dataset is composed of a collection of Tennis matches and their outcomes, provided in multiple files. You should download the data from https://drive.google.com/open?id=1ie6WSl8qkknpSGAFt32Gj7Tf0JAhTLu0 and place the folder somewhere in your Google drive. We provide you the following function *read_data_for_embedding_network*, which reads the files and create a few dictionaries that will help you handle the data later, and a list of instances for training the embedding network. Here is a detailed description of the output of this function:\n",
        "\n",
        "1. player_names - this is a dictionary that maps between a player id and a player name. This player id is unique, and it is given by the dataset owners.\n",
        "2. player_id2index - this is a dictionary that maps between a player id and an index (starting at 0). This index is unique, and it is created by us. It will help us represent the players with a one-hot vector later.\n",
        "3. players_n - an integer representing the number of players we have in the data (this will be the length of a one-hot vector representing a player).\n",
        "4. embedding_data - the dataset to train the embedding network. This dataset is a simple list of lists. Each internal list represents an individual training instance, coomposed of three components: the winner index, the loser index and a binary represnting who scored more aces (a serve that was not responded) during the match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCAYicOchvD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "024500f0-9b62-4c5f-f1c6-21259806ca81"
      },
      "source": [
        "def read_data_for_embedding_network(folder_path):\n",
        "  player_names = {}\n",
        "  player_id2index = {}\n",
        "  players_n = 0\n",
        "  embedding_data = []\n",
        "  pathlist = Path(folder_path).glob('**/*.csv')\n",
        "  for path in pathlist:\n",
        "     path_in_str = str(path)\n",
        "     print(path_in_str)\n",
        "     raw = pd.read_csv(path_in_str, low_memory=False)\n",
        "     for i, match in raw.iterrows():\n",
        "      if match['winner_id'] not in player_names:\n",
        "        player_names[match['winner_id']] = match['winner_name']\n",
        "        player_id2index[match['winner_id']] = players_n\n",
        "        players_n += 1\n",
        "      if match['loser_id'] not in player_names:\n",
        "        player_names[match['loser_id']] = match['loser_name']\n",
        "        player_id2index[match['loser_id']] = players_n\n",
        "        players_n += 1\n",
        "      embedding_data.append([player_id2index[match['winner_id']], \n",
        "                             player_id2index[match['loser_id']], \n",
        "                            int(match['w_ace'] > match['l_ace'])])\n",
        "  return player_names, player_id2index, players_n, embedding_data\n",
        "\n",
        "#TODO - put your own path for the dataset folder\n",
        "player_names, player_id2index, players_n, embedding_data = read_data_for_embedding_network('/content/drive/My Drive/atp-matches-dataset') \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2000.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2002.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2001.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2004.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2003.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2006.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2005.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2007.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2008.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2009.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2010.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2013.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2011.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2012.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2015.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2014.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2017.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hILZZ78u0P0-",
        "colab_type": "text"
      },
      "source": [
        "**Embedding network**\n",
        "\n",
        "Here I code the embedding network. An embedding network in general, is created for learning how to represent an input provided as a one-hot vector. It usually \"projects\" the one-hot vectors on another vector space, which has less dimensions that capture some meaningful information about the inputs. In this case, we will feed the network with the instances we got from the previous function: one hot vector to represent the winner, and ont hot vector to represent the loser. The network will try to predict who scored more aces. I  implement the network, based on the architecture depicted in the followin diagram. **Note - use ReLU activation, only after Linear 1.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVS7BiMi59V5",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://docs.google.com/drawings/d/e/2PACX-1vRp5PeqjF_W-gQmpjcY32hx4n5kWKsySdvLr3avz-bc0kodQbSlvNFuTBX6lcLzS6eS1nwjW0IdVCTY/pub?w=960&h=720)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDy7zfbj3bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - complete the following network based on the diagram that's mentioned above.\n",
        "\n",
        "class EmbeddingsNet(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, players_n, hidden_size):\n",
        "    '''\n",
        "      players_n - the size of the one hot vectors representing a single player\n",
        "      hidden_size - the output size of Linear of winner, Linear of loser and Linear 1 from the diagram\n",
        "      Note - Linear 2's output size should be 2\n",
        "    '''\n",
        "    super(EmbeddingsNet, self).__init__()\n",
        "    self.players_n = players_n\n",
        "    self.hidden_size = hidden_size\n",
        "    self.linear_winner = nn.Linear(players_n,hidden_size)\n",
        "    self.linear_loser = nn.Linear(players_n,hidden_size)\n",
        "    self.linear1 = nn.Linear(2*hidden_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, 2) \n",
        "\n",
        "    \n",
        "  def forward(self, player_1_batch, player_2_batch):\n",
        "    '''\n",
        "      player_1_batch - Bx(number of players) tensor - a batch of winner one hot vectors\n",
        "      player_2_batch - Bx(number of players) tensor - a batch of loser one hot vectors\n",
        "    '''\n",
        "    winner_out = self.linear_winner(player_1_batch)\n",
        "    loser_out = self.linear_loser(player_2_batch)\n",
        "    x = torch.cat((winner_out, loser_out), dim=1)\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rQ3ovtN7ogP",
        "colab_type": "text"
      },
      "source": [
        "**Training the embedding network**\n",
        "\n",
        " Function that trains the embedding network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7czu1v5d7l7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5c92049d-7774-408a-8b5d-59d399215325"
      },
      "source": [
        "# Intializing the embedding network:\n",
        "embedding_net = EmbeddingsNet(players_n, 200).cuda()\n",
        "print(players_n)\n",
        "print(embedding_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2053\n",
            "EmbeddingsNet(\n",
            "  (linear_winner): Linear(in_features=2053, out_features=200, bias=False)\n",
            "  (linear_loser): Linear(in_features=2053, out_features=200, bias=False)\n",
            "  (linear1): Linear(in_features=400, out_features=200, bias=False)\n",
            "  (linear2): Linear(in_features=200, out_features=2, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a1E81jwmWYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - you should implement the following function. You should define \n",
        "# all the relevant tools required to train the network (e.g., optimizer, loss function) \n",
        "# inside this function. Please use Adam optimizer with default parameters.\n",
        "# We suggest that you print the loss function every once in a while so that you \n",
        "# can watch the network improves. \n",
        "\n",
        "def train_embeddings_network(model, data, epochs, batch_size):\n",
        "  '''\n",
        "    model - the embedding_net object\n",
        "    data - the dataset (i.e., embedding_data from above)\n",
        "    epochs - number of epochs to train the network\n",
        "    batch_size - the batch size\n",
        "  '''\n",
        " \n",
        "  trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "        # get the inputs\n",
        "        winner, loser, labels = data\n",
        "\n",
        "        # For GPU\n",
        "        winner = winner.cuda()\n",
        "        loser = loser.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        one_hot_winner =F.one_hot(winner, model.players_n).float()\n",
        "        one_hot_loser =F.one_hot(loser, model.players_n).float()\n",
        "\n",
        "        # forward + backward + optimizer\n",
        "        outputs = model(one_hot_winner, one_hot_loser)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5E3MQtTQh9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcc04e53-2ec0-4fff-f745-0698e395d0a5"
      },
      "source": [
        "# call the training procedure you implemented:\n",
        "train_embeddings_network(embedding_net, embedding_data, 40, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 0.594\n",
            "[1,  4000] loss: 0.558\n",
            "[1,  6000] loss: 0.545\n",
            "[2,  2000] loss: 0.521\n",
            "[2,  4000] loss: 0.528\n",
            "[2,  6000] loss: 0.525\n",
            "[3,  2000] loss: 0.507\n",
            "[3,  4000] loss: 0.512\n",
            "[3,  6000] loss: 0.516\n",
            "[4,  2000] loss: 0.498\n",
            "[4,  4000] loss: 0.504\n",
            "[4,  6000] loss: 0.510\n",
            "[5,  2000] loss: 0.492\n",
            "[5,  4000] loss: 0.501\n",
            "[5,  6000] loss: 0.499\n",
            "[6,  2000] loss: 0.485\n",
            "[6,  4000] loss: 0.486\n",
            "[6,  6000] loss: 0.498\n",
            "[7,  2000] loss: 0.477\n",
            "[7,  4000] loss: 0.483\n",
            "[7,  6000] loss: 0.488\n",
            "[8,  2000] loss: 0.468\n",
            "[8,  4000] loss: 0.478\n",
            "[8,  6000] loss: 0.483\n",
            "[9,  2000] loss: 0.459\n",
            "[9,  4000] loss: 0.472\n",
            "[9,  6000] loss: 0.476\n",
            "[10,  2000] loss: 0.456\n",
            "[10,  4000] loss: 0.468\n",
            "[10,  6000] loss: 0.470\n",
            "[11,  2000] loss: 0.450\n",
            "[11,  4000] loss: 0.458\n",
            "[11,  6000] loss: 0.463\n",
            "[12,  2000] loss: 0.444\n",
            "[12,  4000] loss: 0.453\n",
            "[12,  6000] loss: 0.458\n",
            "[13,  2000] loss: 0.439\n",
            "[13,  4000] loss: 0.442\n",
            "[13,  6000] loss: 0.447\n",
            "[14,  2000] loss: 0.433\n",
            "[14,  4000] loss: 0.435\n",
            "[14,  6000] loss: 0.447\n",
            "[15,  2000] loss: 0.423\n",
            "[15,  4000] loss: 0.429\n",
            "[15,  6000] loss: 0.445\n",
            "[16,  2000] loss: 0.415\n",
            "[16,  4000] loss: 0.432\n",
            "[16,  6000] loss: 0.435\n",
            "[17,  2000] loss: 0.414\n",
            "[17,  4000] loss: 0.420\n",
            "[17,  6000] loss: 0.429\n",
            "[18,  2000] loss: 0.404\n",
            "[18,  4000] loss: 0.420\n",
            "[18,  6000] loss: 0.424\n",
            "[19,  2000] loss: 0.401\n",
            "[19,  4000] loss: 0.409\n",
            "[19,  6000] loss: 0.426\n",
            "[20,  2000] loss: 0.394\n",
            "[20,  4000] loss: 0.410\n",
            "[20,  6000] loss: 0.414\n",
            "[21,  2000] loss: 0.393\n",
            "[21,  4000] loss: 0.400\n",
            "[21,  6000] loss: 0.413\n",
            "[22,  2000] loss: 0.385\n",
            "[22,  4000] loss: 0.400\n",
            "[22,  6000] loss: 0.409\n",
            "[23,  2000] loss: 0.386\n",
            "[23,  4000] loss: 0.393\n",
            "[23,  6000] loss: 0.407\n",
            "[24,  2000] loss: 0.379\n",
            "[24,  4000] loss: 0.388\n",
            "[24,  6000] loss: 0.400\n",
            "[25,  2000] loss: 0.378\n",
            "[25,  4000] loss: 0.387\n",
            "[25,  6000] loss: 0.397\n",
            "[26,  2000] loss: 0.369\n",
            "[26,  4000] loss: 0.388\n",
            "[26,  6000] loss: 0.392\n",
            "[27,  2000] loss: 0.370\n",
            "[27,  4000] loss: 0.380\n",
            "[27,  6000] loss: 0.393\n",
            "[28,  2000] loss: 0.362\n",
            "[28,  4000] loss: 0.378\n",
            "[28,  6000] loss: 0.389\n",
            "[29,  2000] loss: 0.358\n",
            "[29,  4000] loss: 0.375\n",
            "[29,  6000] loss: 0.387\n",
            "[30,  2000] loss: 0.363\n",
            "[30,  4000] loss: 0.368\n",
            "[30,  6000] loss: 0.383\n",
            "[31,  2000] loss: 0.351\n",
            "[31,  4000] loss: 0.372\n",
            "[31,  6000] loss: 0.380\n",
            "[32,  2000] loss: 0.351\n",
            "[32,  4000] loss: 0.366\n",
            "[32,  6000] loss: 0.382\n",
            "[33,  2000] loss: 0.352\n",
            "[33,  4000] loss: 0.359\n",
            "[33,  6000] loss: 0.378\n",
            "[34,  2000] loss: 0.353\n",
            "[34,  4000] loss: 0.354\n",
            "[34,  6000] loss: 0.380\n",
            "[35,  2000] loss: 0.357\n",
            "[35,  4000] loss: 0.361\n",
            "[35,  6000] loss: 0.361\n",
            "[36,  2000] loss: 0.351\n",
            "[36,  4000] loss: 0.355\n",
            "[36,  6000] loss: 0.371\n",
            "[37,  2000] loss: 0.344\n",
            "[37,  4000] loss: 0.354\n",
            "[37,  6000] loss: 0.369\n",
            "[38,  2000] loss: 0.343\n",
            "[38,  4000] loss: 0.353\n",
            "[38,  6000] loss: 0.367\n",
            "[39,  2000] loss: 0.344\n",
            "[39,  4000] loss: 0.355\n",
            "[39,  6000] loss: 0.361\n",
            "[40,  2000] loss: 0.340\n",
            "[40,  4000] loss: 0.350\n",
            "[40,  6000] loss: 0.360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nieSTHf9a9b",
        "colab_type": "text"
      },
      "source": [
        "**Extracting embeddings**\n",
        "\n",
        "In this section, I extracted the player representation from the embedding network. In fact, we don't realy care about the quality of prediction of the embedding network. We care more about the vector representation it learned for every player. The representation is in fact the weight matrix of the two first linear layers that you created for each player of a given player pair. Each such weight matrix is of size (hidden_size X number_of_players). You can get that matrix of each of the two Linear layers, by calling their *parameters* function. This function returns a Python generator, and the first element is your matrix. Note that since you have two such layers, you will have two matrices; you are requested to add the two matrices together and divide by 2, and then you get a new matrix which will represent the players. This matrix, say M, will be of shape (hidden_size X number_of_players), so **M[:, 3]** for example, is a hidden_size vector representing player of index 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJLOoKkrVRCb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "0d0ff85f-5f76-4e64-f423-8135c0e0ed49"
      },
      "source": [
        "# preparing the embedding matrix, let's call it player_embedding\n",
        "params = embedding_net.parameters()\n",
        "matricies = [param for param in params]\n",
        "# weight matrix of the first two linear layers:\n",
        "mat1=matricies[0]\n",
        "mat2=matricies[2]\n",
        "print(\"this is matrix shapes\")\n",
        "print(\"matrix size index 0 :\" ,matricies[0].shape)\n",
        "print(\"matrix size index 1 :\" ,matricies[1].shape)\n",
        "print(\"matrix size index 2 :\" ,matricies[2].shape)\n",
        "print(\"matrix size index 3 :\" ,matricies[3].shape)\n",
        "print(\"matrix size index 4 :\" ,matricies[4].shape)\n",
        "print(\"matrix size index 5 :\" ,matricies[5].shape)\n",
        "print(\"matrix size index 6 :\" ,matricies[6].shape)\n",
        "print(\"matrix size index 7 :\" ,matricies[7].shape)\n",
        "\n",
        "\n",
        "player_embedding = (mat1 + mat2)/2\n",
        "print(\"Tensor size:\" ,player_embedding.shape)\n",
        "#print(player_embedding[:,3].shape)\n",
        "print(player_embedding)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is matrix shapes\n",
            "matrix size index 0 : torch.Size([200, 2053])\n",
            "matrix size index 1 : torch.Size([200, 2053])\n",
            "matrix size index 2 : torch.Size([200, 400])\n",
            "matrix size index 3 : torch.Size([2, 200])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7f492fa6f5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix size index 2 :\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmatricies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix size index 3 :\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmatricies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix size index 4 :\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmatricies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix size index 5 :\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmatricies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix size index 6 :\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmatricies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaXcfb5XA-Dt",
        "colab_type": "text"
      },
      "source": [
        "**Prediction network**\n",
        "\n",
        "Now, we are ready to implement a simple network that predicts who wins a match, given two players and some additional match-leavel features. Please implement PredictionNetwork based on the following diagram. Each player is provided with it's embedding from the player_embedding matrix you calculated above. **Note - use ReLU after every Linear layer (except the last one of course).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCCqGCVpC4KY",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://docs.google.com/drawings/d/e/2PACX-1vTCAlmaool-UQ9be8wSdjjgOq0VaCiYsXwsHw0HDxZRXTkp6vBm08Ma8sQha-cKDjjeZhfzY2qT8if8/pub?w=960&h=720)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IXZkqLBZHnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PredictionNetwork(torch.nn.Module):\n",
        "  def __init__(self, players_embedding_size, match_feature_size, hidden_size1, hidden_size2, hidden_size3):\n",
        "    '''\n",
        "      players_embedding_size - the length of the vector representing each player \n",
        "      (basically, the hidden_size of the embedding network)\n",
        "      match_feature_size - the number of match-level features we use\n",
        "      hidden_size1 - the output size of all the three first layers (Linear player 1, \n",
        "      Linear player 2, and Linear features)\n",
        "      hidden_size2 - the output size of Linear 1\n",
        "      hidden_size3 - the output size of Linear 2\n",
        "    '''\n",
        "    super(PredictionNetwork, self).__init__()\n",
        "    self.players_embedding_size = players_embedding_size\n",
        "    self.match_feature_size = match_feature_size\n",
        "    self.hidden_size1= hidden_size1\n",
        "    self.hidden_size2 = hidden_size2\n",
        "    self.hidden_size3 = hidden_size3\n",
        "    # net architecture:\n",
        "    self.linear_player1 = nn.Linear(players_embedding_size,hidden_size1)\n",
        "    self.linear_player2 = nn.Linear(players_embedding_size,hidden_size1)\n",
        "    self.linear_feature = nn.Linear(match_feature_size,hidden_size1)\n",
        "    self.linear1 = nn.Linear(3*hidden_size1,hidden_size2)\n",
        "    self.linear2 = nn.Linear(hidden_size2,hidden_size3)\n",
        "    self.linear3 = nn.Linear(hidden_size3,2)\n",
        "\n",
        "  def forward(self, player1_batch, player2_batch, match_features_batch):\n",
        "    '''\n",
        "      player1_batch - Bx(players_embedding_size) tensor - a batch of player 1 embedding vectors\n",
        "      player2_batch - Bx(players_embedding_size) tensor - a batch of player 2 embedding vectors\n",
        "      match_features_batch - Bx(match_feature_size) tensor - a batch of match-level features\n",
        "    ''' \n",
        "    player1_batch = F.relu(self.linear_player1(player1_batch))\n",
        "    player2_batch = F.relu(self.linear_player2(player2_batch))\n",
        "    match_features_batch = F.relu(self.linear_feature(match_features_batch))\n",
        "    # concat the 3 linear layers into 1 dimension vector\n",
        "    x=torch.cat((player1_batch,player2_batch,match_features_batch),1)\n",
        "    x=F.relu(self.linear1(x))\n",
        "    x=F.relu(self.linear2(x))\n",
        "    x=self.linear3(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWmy7IxE9Vj",
        "colab_type": "text"
      },
      "source": [
        "The following function, **prepare_match_dataset**, prepares the dataset of instances required to train your prediction network. We randomize the position of the winner and loser in each instance, to avoid converging to position based prediction. As you can see, we only use only a single match-level feature for now: draw_size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytX91LB4e60h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "e9dc1539-5f7f-42ae-c09c-834f693934e9"
      },
      "source": [
        "def prepare_match_dataset(folder_path, player_embedding):\n",
        "  X = []\n",
        "  y = []\n",
        "  surface_dictionary = {}\n",
        "  pathlist = Path(folder_path).glob('**/*.csv')\n",
        "  for path in pathlist:\n",
        "    path_in_str = str(path)\n",
        "    print(path_in_str)\n",
        "    raw = pd.read_csv(path_in_str, low_memory=False)\n",
        "    for i, match in raw.iterrows():\n",
        "      instance = []\n",
        "      features = []\n",
        "      features.append(match['draw_size'])\n",
        "      winner_embedding = player_embedding[:, player_id2index[match['winner_id']]].cpu().detach().numpy()\n",
        "      loser_embedding = player_embedding[:, player_id2index[match['loser_id']]].cpu().detach().numpy()\n",
        "      target = -1\n",
        "      if random.random() < 0.5:\n",
        "         instance.append(winner_embedding)\n",
        "         instance.append(loser_embedding)\n",
        "         target = 0\n",
        "      else:\n",
        "         instance.append(loser_embedding)\n",
        "         instance.append(winner_embedding)\n",
        "         target = 1\n",
        "      instance.append(features)\n",
        "      X.append(instance)\n",
        "      y.append(target)\n",
        "\n",
        "\n",
        "  return X, y\n",
        "\n",
        "\n",
        "#TODO - put your own path for the dataset folder\n",
        "X, y = prepare_match_dataset('/content/drive/My Drive/atp-matches-dataset', player_embedding) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2000.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2002.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2001.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2004.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2003.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2006.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2005.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2007.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2008.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2009.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2010.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2013.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2011.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2012.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2015.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2014.csv\n",
            "/content/drive/My Drive/atp-matches-dataset/atp_matches_2017.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPJvKH9KFpMI",
        "colab_type": "text"
      },
      "source": [
        "**Training the prediction network**\n",
        "\n",
        "Now, you should write code to train the prediction network using the X,y dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQJ6BJNhffy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - split the data into train and test. Please allocate 25% of the data for test, using random_state=42\n",
        "# X = data the players\n",
        "# y = labels 0/1 ( win or lose )\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6LK_Fq-AYKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - you should implement the following train_match_network function. You should define \n",
        "# all the relevant tools required to train the network (e.g., optimizer, loss function) \n",
        "# inside this function. Please use Adam optimizer with default parameters.\n",
        "# You should print and plot the loss function for the train data every once in a while, \n",
        "# as well as accuracy for the test data (every epoch) so that you \n",
        "# can watch the network improves.\n",
        "\n",
        "\n",
        "# Student's note:\n",
        "# It is not clear whether we should also PLOT the loss and accuracy \"once in a while\".\n",
        "# We have thus decided to only print it, and then plot it after the training ends.\n",
        "\n",
        "# Accuracy function\n",
        "def Calculate_Accuracy(testloader,model,epoch):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # for GPU\n",
        "        players1=inputs[0].cuda()\n",
        "        players2=inputs[1].cuda()\n",
        "        feature=inputs[2][0].cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # create 1 vector array in 'match_feature_size' size and match data\n",
        "        feature = feature.reshape(-1,model.match_feature_size).float()\n",
        "        \n",
        "        # calculate accuracy\n",
        "        outputs = model(players1,players2,feature)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "  accuracy = (100 * correct / total);\n",
        "  #print accuracy per epoch\n",
        "  print('Epoch %d Accuracy: %d %%' % (epoch+1,accuracy )) \n",
        "  return accuracy;\n",
        "\n",
        "def train_match_network(model, X_train, X_test, y_train, y_test, epochs, batch_size):\n",
        "  '''\n",
        "    X_train - the training data\n",
        "    X_test - the testing data\n",
        "    y_train - the target of the training data\n",
        "    y_test - the target of the testing data\n",
        "    epochs - number of epochs\n",
        "    batch_size - the batch size\n",
        "  '''\n",
        "  train_data=[[x,y] for x,y in zip(X_train,y_train)]\n",
        "  test_data=[[x,y] for x,y in zip(X_test,y_test)]\n",
        "  trainloader=torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  testloader=torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=False)\n",
        "\n",
        "  # define the loss arrays\n",
        "  train_loss = []\n",
        "  accuracy = []\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # for GPU\n",
        "        players1 =inputs[0].cuda()\n",
        "        players2 =inputs[1].cuda()\n",
        "        feature = inputs[2][0].cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # create 1 vector array in 'match_feature_size' size and match data\n",
        "        feature = feature.reshape(-1,model.match_feature_size).float()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the model's output\n",
        "        outputs = model(players1,players2,feature)\n",
        "\n",
        "        # Define the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Take an optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Rolling loss\n",
        "        running_loss += loss.item()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Print the batch loss every 2000th batch\n",
        "        # if((i+1) % 2000 == 0):\n",
        "        #   print('[%d, %5d] loss: %.3f' %\n",
        "        #     (epoch + 1, (i + 1), running_loss / (i+1)))\n",
        "\n",
        "        if((i+1) % 2000 == 0):\n",
        "          print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch + 1, (i + 1), running_loss / 2000))\n",
        "          running_loss = 0.0\n",
        "\n",
        "    # Calculate the accuracy on the test set\n",
        "    accuracy.append(Calculate_Accuracy(testloader,model,epoch))\n",
        "\n",
        "    # Add the epoch's loss\n",
        "    train_loss.append(epoch_loss / len(trainloader))\n",
        "\n",
        "  return train_loss,accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHxvKBIfZ_Ix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ceae673-f25f-4d94-fbce-d1e338ca26ae"
      },
      "source": [
        "# TODO - train your network\n",
        "tennisMatchModel = PredictionNetwork(200, 1, 1024, 700, 200).cuda()\n",
        "train_loss,accuracy = train_match_network(tennisMatchModel, X_train, X_test, y_train, y_test, 40, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 0.797\n",
            "[1,  4000] loss: 0.732\n",
            "[1,  6000] loss: 0.695\n",
            "[1,  8000] loss: 0.695\n",
            "[1, 10000] loss: 0.706\n",
            "[1, 12000] loss: 0.724\n",
            "[1, 14000] loss: 0.696\n",
            "[1, 16000] loss: 0.694\n",
            "[1, 18000] loss: 0.695\n",
            "Epoch 1 Accuracy: 50 %\n",
            "[2,  2000] loss: 0.694\n",
            "[2,  4000] loss: 0.692\n",
            "[2,  6000] loss: 0.694\n",
            "[2,  8000] loss: 0.692\n",
            "[2, 10000] loss: 0.696\n",
            "[2, 12000] loss: 0.690\n",
            "[2, 14000] loss: 0.693\n",
            "[2, 16000] loss: 0.688\n",
            "[2, 18000] loss: 0.693\n",
            "Epoch 2 Accuracy: 50 %\n",
            "[3,  2000] loss: 0.684\n",
            "[3,  4000] loss: 0.688\n",
            "[3,  6000] loss: 0.683\n",
            "[3,  8000] loss: 0.692\n",
            "[3, 10000] loss: 0.691\n",
            "[3, 12000] loss: 0.690\n",
            "[3, 14000] loss: 0.691\n",
            "[3, 16000] loss: 0.688\n",
            "[3, 18000] loss: 0.691\n",
            "Epoch 3 Accuracy: 50 %\n",
            "[4,  2000] loss: 0.683\n",
            "[4,  4000] loss: 0.684\n",
            "[4,  6000] loss: 0.687\n",
            "[4,  8000] loss: 0.686\n",
            "[4, 10000] loss: 0.685\n",
            "[4, 12000] loss: 0.681\n",
            "[4, 14000] loss: 0.680\n",
            "[4, 16000] loss: 0.684\n",
            "[4, 18000] loss: 0.681\n",
            "Epoch 4 Accuracy: 56 %\n",
            "[5,  2000] loss: 0.679\n",
            "[5,  4000] loss: 0.663\n",
            "[5,  6000] loss: 0.661\n",
            "[5,  8000] loss: 0.653\n",
            "[5, 10000] loss: 0.647\n",
            "[5, 12000] loss: 0.640\n",
            "[5, 14000] loss: 0.649\n",
            "[5, 16000] loss: 0.642\n",
            "[5, 18000] loss: 0.650\n",
            "Epoch 5 Accuracy: 62 %\n",
            "[6,  2000] loss: 0.642\n",
            "[6,  4000] loss: 0.644\n",
            "[6,  6000] loss: 0.642\n",
            "[6,  8000] loss: 0.637\n",
            "[6, 10000] loss: 0.638\n",
            "[6, 12000] loss: 0.639\n",
            "[6, 14000] loss: 0.635\n",
            "[6, 16000] loss: 0.645\n",
            "[6, 18000] loss: 0.628\n",
            "Epoch 6 Accuracy: 64 %\n",
            "[7,  2000] loss: 0.634\n",
            "[7,  4000] loss: 0.630\n",
            "[7,  6000] loss: 0.629\n",
            "[7,  8000] loss: 0.628\n",
            "[7, 10000] loss: 0.636\n",
            "[7, 12000] loss: 0.634\n",
            "[7, 14000] loss: 0.635\n",
            "[7, 16000] loss: 0.628\n",
            "[7, 18000] loss: 0.625\n",
            "Epoch 7 Accuracy: 65 %\n",
            "[8,  2000] loss: 0.633\n",
            "[8,  4000] loss: 0.630\n",
            "[8,  6000] loss: 0.628\n",
            "[8,  8000] loss: 0.628\n",
            "[8, 10000] loss: 0.638\n",
            "[8, 12000] loss: 0.617\n",
            "[8, 14000] loss: 0.638\n",
            "[8, 16000] loss: 0.651\n",
            "[8, 18000] loss: 0.630\n",
            "Epoch 8 Accuracy: 62 %\n",
            "[9,  2000] loss: 0.620\n",
            "[9,  4000] loss: 0.631\n",
            "[9,  6000] loss: 0.613\n",
            "[9,  8000] loss: 0.627\n",
            "[9, 10000] loss: 0.636\n",
            "[9, 12000] loss: 0.624\n",
            "[9, 14000] loss: 0.634\n",
            "[9, 16000] loss: 0.614\n",
            "[9, 18000] loss: 0.631\n",
            "Epoch 9 Accuracy: 64 %\n",
            "[10,  2000] loss: 0.622\n",
            "[10,  4000] loss: 0.617\n",
            "[10,  6000] loss: 0.620\n",
            "[10,  8000] loss: 0.618\n",
            "[10, 10000] loss: 0.623\n",
            "[10, 12000] loss: 0.621\n",
            "[10, 14000] loss: 0.621\n",
            "[10, 16000] loss: 0.632\n",
            "[10, 18000] loss: 0.628\n",
            "Epoch 10 Accuracy: 64 %\n",
            "[11,  2000] loss: 0.621\n",
            "[11,  4000] loss: 0.617\n",
            "[11,  6000] loss: 0.621\n",
            "[11,  8000] loss: 0.637\n",
            "[11, 10000] loss: 0.607\n",
            "[11, 12000] loss: 0.618\n",
            "[11, 14000] loss: 0.630\n",
            "[11, 16000] loss: 0.633\n",
            "[11, 18000] loss: 0.648\n",
            "Epoch 11 Accuracy: 64 %\n",
            "[12,  2000] loss: 0.625\n",
            "[12,  4000] loss: 0.621\n",
            "[12,  6000] loss: 0.623\n",
            "[12,  8000] loss: 0.625\n",
            "[12, 10000] loss: 0.629\n",
            "[12, 12000] loss: 0.621\n",
            "[12, 14000] loss: 0.614\n",
            "[12, 16000] loss: 0.636\n",
            "[12, 18000] loss: 0.616\n",
            "Epoch 12 Accuracy: 65 %\n",
            "[13,  2000] loss: 0.621\n",
            "[13,  4000] loss: 0.616\n",
            "[13,  6000] loss: 0.602\n",
            "[13,  8000] loss: 0.633\n",
            "[13, 10000] loss: 0.613\n",
            "[13, 12000] loss: 0.626\n",
            "[13, 14000] loss: 0.625\n",
            "[13, 16000] loss: 0.623\n",
            "[13, 18000] loss: 0.624\n",
            "Epoch 13 Accuracy: 65 %\n",
            "[14,  2000] loss: 0.614\n",
            "[14,  4000] loss: 0.617\n",
            "[14,  6000] loss: 0.620\n",
            "[14,  8000] loss: 0.629\n",
            "[14, 10000] loss: 0.628\n",
            "[14, 12000] loss: 0.620\n",
            "[14, 14000] loss: 0.626\n",
            "[14, 16000] loss: 0.619\n",
            "[14, 18000] loss: 0.615\n",
            "Epoch 14 Accuracy: 64 %\n",
            "[15,  2000] loss: 0.616\n",
            "[15,  4000] loss: 0.620\n",
            "[15,  6000] loss: 0.625\n",
            "[15,  8000] loss: 0.617\n",
            "[15, 10000] loss: 0.611\n",
            "[15, 12000] loss: 0.618\n",
            "[15, 14000] loss: 0.613\n",
            "[15, 16000] loss: 0.620\n",
            "[15, 18000] loss: 0.608\n",
            "Epoch 15 Accuracy: 65 %\n",
            "[16,  2000] loss: 0.628\n",
            "[16,  4000] loss: 0.616\n",
            "[16,  6000] loss: 0.622\n",
            "[16,  8000] loss: 0.622\n",
            "[16, 10000] loss: 0.617\n",
            "[16, 12000] loss: 0.615\n",
            "[16, 14000] loss: 0.635\n",
            "[16, 16000] loss: 0.608\n",
            "[16, 18000] loss: 0.624\n",
            "Epoch 16 Accuracy: 65 %\n",
            "[17,  2000] loss: 0.621\n",
            "[17,  4000] loss: 0.603\n",
            "[17,  6000] loss: 0.626\n",
            "[17,  8000] loss: 0.614\n",
            "[17, 10000] loss: 0.625\n",
            "[17, 12000] loss: 0.628\n",
            "[17, 14000] loss: 0.632\n",
            "[17, 16000] loss: 0.619\n",
            "[17, 18000] loss: 0.611\n",
            "Epoch 17 Accuracy: 65 %\n",
            "[18,  2000] loss: 0.613\n",
            "[18,  4000] loss: 0.611\n",
            "[18,  6000] loss: 0.614\n",
            "[18,  8000] loss: 0.618\n",
            "[18, 10000] loss: 0.619\n",
            "[18, 12000] loss: 0.623\n",
            "[18, 14000] loss: 0.627\n",
            "[18, 16000] loss: 0.613\n",
            "[18, 18000] loss: 0.620\n",
            "Epoch 18 Accuracy: 65 %\n",
            "[19,  2000] loss: 0.633\n",
            "[19,  4000] loss: 0.622\n",
            "[19,  6000] loss: 0.628\n",
            "[19,  8000] loss: 0.628\n",
            "[19, 10000] loss: 0.624\n",
            "[19, 12000] loss: 0.628\n",
            "[19, 14000] loss: 0.635\n",
            "[19, 16000] loss: 0.626\n",
            "[19, 18000] loss: 0.618\n",
            "Epoch 19 Accuracy: 65 %\n",
            "[20,  2000] loss: 0.623\n",
            "[20,  4000] loss: 0.618\n",
            "[20,  6000] loss: 0.623\n",
            "[20,  8000] loss: 0.618\n",
            "[20, 10000] loss: 0.627\n",
            "[20, 12000] loss: 0.615\n",
            "[20, 14000] loss: 0.619\n",
            "[20, 16000] loss: 0.637\n",
            "[20, 18000] loss: 0.611\n",
            "Epoch 20 Accuracy: 63 %\n",
            "[21,  2000] loss: 0.605\n",
            "[21,  4000] loss: 0.602\n",
            "[21,  6000] loss: 0.629\n",
            "[21,  8000] loss: 0.620\n",
            "[21, 10000] loss: 0.624\n",
            "[21, 12000] loss: 0.606\n",
            "[21, 14000] loss: 0.620\n",
            "[21, 16000] loss: 0.634\n",
            "[21, 18000] loss: 0.611\n",
            "Epoch 21 Accuracy: 65 %\n",
            "[22,  2000] loss: 0.615\n",
            "[22,  4000] loss: 0.614\n",
            "[22,  6000] loss: 0.613\n",
            "[22,  8000] loss: 0.620\n",
            "[22, 10000] loss: 0.609\n",
            "[22, 12000] loss: 0.619\n",
            "[22, 14000] loss: 0.601\n",
            "[22, 16000] loss: 0.604\n",
            "[22, 18000] loss: 0.612\n",
            "Epoch 22 Accuracy: 65 %\n",
            "[23,  2000] loss: 0.610\n",
            "[23,  4000] loss: 0.606\n",
            "[23,  6000] loss: 0.625\n",
            "[23,  8000] loss: 0.611\n",
            "[23, 10000] loss: 0.610\n",
            "[23, 12000] loss: 0.619\n",
            "[23, 14000] loss: 0.625\n",
            "[23, 16000] loss: 0.625\n",
            "[23, 18000] loss: 0.623\n",
            "Epoch 23 Accuracy: 64 %\n",
            "[24,  2000] loss: 0.611\n",
            "[24,  4000] loss: 0.610\n",
            "[24,  6000] loss: 0.605\n",
            "[24,  8000] loss: 0.619\n",
            "[24, 10000] loss: 0.621\n",
            "[24, 12000] loss: 0.619\n",
            "[24, 14000] loss: 0.638\n",
            "[24, 16000] loss: 0.615\n",
            "[24, 18000] loss: 0.619\n",
            "Epoch 24 Accuracy: 65 %\n",
            "[25,  2000] loss: 0.613\n",
            "[25,  4000] loss: 0.622\n",
            "[25,  6000] loss: 0.599\n",
            "[25,  8000] loss: 0.616\n",
            "[25, 10000] loss: 0.605\n",
            "[25, 12000] loss: 0.616\n",
            "[25, 14000] loss: 0.618\n",
            "[25, 16000] loss: 0.625\n",
            "[25, 18000] loss: 0.629\n",
            "Epoch 25 Accuracy: 63 %\n",
            "[26,  2000] loss: 0.604\n",
            "[26,  4000] loss: 0.603\n",
            "[26,  6000] loss: 0.613\n",
            "[26,  8000] loss: 0.612\n",
            "[26, 10000] loss: 0.617\n",
            "[26, 12000] loss: 0.633\n",
            "[26, 14000] loss: 0.628\n",
            "[26, 16000] loss: 0.622\n",
            "[26, 18000] loss: 0.615\n",
            "Epoch 26 Accuracy: 65 %\n",
            "[27,  2000] loss: 0.618\n",
            "[27,  4000] loss: 0.619\n",
            "[27,  6000] loss: 0.619\n",
            "[27,  8000] loss: 0.608\n",
            "[27, 10000] loss: 0.609\n",
            "[27, 12000] loss: 0.609\n",
            "[27, 14000] loss: 0.607\n",
            "[27, 16000] loss: 0.610\n",
            "[27, 18000] loss: 0.619\n",
            "Epoch 27 Accuracy: 65 %\n",
            "[28,  2000] loss: 0.607\n",
            "[28,  4000] loss: 0.603\n",
            "[28,  6000] loss: 0.605\n",
            "[28,  8000] loss: 0.611\n",
            "[28, 10000] loss: 0.611\n",
            "[28, 12000] loss: 0.626\n",
            "[28, 14000] loss: 0.611\n",
            "[28, 16000] loss: 0.629\n",
            "[28, 18000] loss: 0.617\n",
            "Epoch 28 Accuracy: 65 %\n",
            "[29,  2000] loss: 0.612\n",
            "[29,  4000] loss: 0.620\n",
            "[29,  6000] loss: 0.616\n",
            "[29,  8000] loss: 0.625\n",
            "[29, 10000] loss: 0.611\n",
            "[29, 12000] loss: 0.621\n",
            "[29, 14000] loss: 0.606\n",
            "[29, 16000] loss: 0.618\n",
            "[29, 18000] loss: 0.610\n",
            "Epoch 29 Accuracy: 64 %\n",
            "[30,  2000] loss: 0.610\n",
            "[30,  4000] loss: 0.610\n",
            "[30,  6000] loss: 0.613\n",
            "[30,  8000] loss: 0.612\n",
            "[30, 10000] loss: 0.610\n",
            "[30, 12000] loss: 0.627\n",
            "[30, 14000] loss: 0.637\n",
            "[30, 16000] loss: 0.617\n",
            "[30, 18000] loss: 0.627\n",
            "Epoch 30 Accuracy: 62 %\n",
            "[31,  2000] loss: 0.619\n",
            "[31,  4000] loss: 0.620\n",
            "[31,  6000] loss: 0.634\n",
            "[31,  8000] loss: 0.615\n",
            "[31, 10000] loss: 0.615\n",
            "[31, 12000] loss: 0.619\n",
            "[31, 14000] loss: 0.619\n",
            "[31, 16000] loss: 0.617\n",
            "[31, 18000] loss: 0.625\n",
            "Epoch 31 Accuracy: 64 %\n",
            "[32,  2000] loss: 0.620\n",
            "[32,  4000] loss: 0.607\n",
            "[32,  6000] loss: 0.622\n",
            "[32,  8000] loss: 0.626\n",
            "[32, 10000] loss: 0.615\n",
            "[32, 12000] loss: 0.625\n",
            "[32, 14000] loss: 0.621\n",
            "[32, 16000] loss: 0.610\n",
            "[32, 18000] loss: 0.614\n",
            "Epoch 32 Accuracy: 64 %\n",
            "[33,  2000] loss: 0.616\n",
            "[33,  4000] loss: 0.622\n",
            "[33,  6000] loss: 0.617\n",
            "[33,  8000] loss: 0.612\n",
            "[33, 10000] loss: 0.621\n",
            "[33, 12000] loss: 0.607\n",
            "[33, 14000] loss: 0.618\n",
            "[33, 16000] loss: 0.616\n",
            "[33, 18000] loss: 0.613\n",
            "Epoch 33 Accuracy: 63 %\n",
            "[34,  2000] loss: 0.611\n",
            "[34,  4000] loss: 0.609\n",
            "[34,  6000] loss: 0.617\n",
            "[34,  8000] loss: 0.616\n",
            "[34, 10000] loss: 0.606\n",
            "[34, 12000] loss: 0.616\n",
            "[34, 14000] loss: 0.622\n",
            "[34, 16000] loss: 0.622\n",
            "[34, 18000] loss: 0.632\n",
            "Epoch 34 Accuracy: 64 %\n",
            "[35,  2000] loss: 0.607\n",
            "[35,  4000] loss: 0.608\n",
            "[35,  6000] loss: 0.612\n",
            "[35,  8000] loss: 0.624\n",
            "[35, 10000] loss: 0.622\n",
            "[35, 12000] loss: 0.624\n",
            "[35, 14000] loss: 0.647\n",
            "[35, 16000] loss: 0.608\n",
            "[35, 18000] loss: 0.617\n",
            "Epoch 35 Accuracy: 64 %\n",
            "[36,  2000] loss: 0.621\n",
            "[36,  4000] loss: 0.613\n",
            "[36,  6000] loss: 0.613\n",
            "[36,  8000] loss: 0.613\n",
            "[36, 10000] loss: 0.619\n",
            "[36, 12000] loss: 0.611\n",
            "[36, 14000] loss: 0.618\n",
            "[36, 16000] loss: 0.614\n",
            "[36, 18000] loss: 0.624\n",
            "Epoch 36 Accuracy: 65 %\n",
            "[37,  2000] loss: 0.615\n",
            "[37,  4000] loss: 0.600\n",
            "[37,  6000] loss: 0.605\n",
            "[37,  8000] loss: 0.627\n",
            "[37, 10000] loss: 0.613\n",
            "[37, 12000] loss: 0.617\n",
            "[37, 14000] loss: 0.618\n",
            "[37, 16000] loss: 0.609\n",
            "[37, 18000] loss: 0.615\n",
            "Epoch 37 Accuracy: 65 %\n",
            "[38,  2000] loss: 0.596\n",
            "[38,  4000] loss: 0.605\n",
            "[38,  6000] loss: 0.622\n",
            "[38,  8000] loss: 0.635\n",
            "[38, 10000] loss: 0.602\n",
            "[38, 12000] loss: 0.623\n",
            "[38, 14000] loss: 0.605\n",
            "[38, 16000] loss: 0.615\n",
            "[38, 18000] loss: 0.606\n",
            "Epoch 38 Accuracy: 65 %\n",
            "[39,  2000] loss: 0.606\n",
            "[39,  4000] loss: 0.599\n",
            "[39,  6000] loss: 0.614\n",
            "[39,  8000] loss: 0.611\n",
            "[39, 10000] loss: 0.613\n",
            "[39, 12000] loss: 0.616\n",
            "[39, 14000] loss: 0.621\n",
            "[39, 16000] loss: 0.607\n",
            "[39, 18000] loss: 0.617\n",
            "Epoch 39 Accuracy: 64 %\n",
            "[40,  2000] loss: 0.619\n",
            "[40,  4000] loss: 0.609\n",
            "[40,  6000] loss: 0.611\n",
            "[40,  8000] loss: 0.615\n",
            "[40, 10000] loss: 0.625\n",
            "[40, 12000] loss: 0.613\n",
            "[40, 14000] loss: 0.631\n",
            "[40, 16000] loss: 0.652\n",
            "[40, 18000] loss: 0.644\n",
            "Epoch 40 Accuracy: 63 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDn1NM6KL80J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "b1b0f559-5697-43b1-ff1e-5a36f915a474"
      },
      "source": [
        "# ---------------------------------------------- #\n",
        "#                     Loss\n",
        "# ---------------------------------------------- #\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.plot(train_loss ,label = 'train',c ='blue')\n",
        "  #plt.plot(x, test_loss,label = 'test',c='red')\n",
        "  plt.title('loss per epoch')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()\n",
        "\n",
        "# ---------------------------------------------- #\n",
        "#                     Accuracy\n",
        "# ---------------------------------------------- #\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.plot(accuracy,label = 'Accuracy',c ='blue')\n",
        "  plt.title('Accuracy per Epoch')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAFNCAYAAABFbcjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTERERCSNKJyJiIiIpBGFMxEREZE08v8B30xFYHxPmTMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFNCAYAAADRi2EuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wfIt9wIsYL9QAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}